# Epic 001 Retrospective - Core NL-to-SQL Query Application

**Date:** 2025-10-02
**Facilitator:** Bob (Scrum Master)
**Participants:** Sarah (PO), Bob (SM), Amelia (Dev), Murat (TEA), Winston (Architect), Mary (Analyst)

---

## EPIC 001 SUMMARY

**Delivery Metrics:**
- **Completed:** 8/8 stories (100%)
- **Velocity:** 8 stories delivered
- **Duration:** 1 sprint (October 1-2, 2025)
- **Average velocity:** All stories completed in 2-day sprint

**Quality & Technical:**
- **Blockers encountered:** 8+ significant blockers
  - Windows port binding (Story 1.1)
  - Database session lifecycle bugs (Story 1.8)
  - GPT-5 Nano API parameter requirements (Story 1.8)
  - Multiple SQLAlchemy 2.0 compatibility issues
- **Technical debt items:** ~15 items logged across stories
- **Test coverage:** 100% (19/19 integration tests + 51 unit tests passing in Story 1.8)
- **Production incidents:** 0 (demo/POC environment)

**Business Outcomes:**
- **Goals achieved:** 8/8 core functional requirements delivered
- **Success criteria:** All 4 mandatory query types working end-to-end
- **Stakeholder feedback:** Pending

---

## NEXT EPIC PREVIEW: Epic 002 - Ragas Evaluation & Reporting

**Dependencies on Epic 001:**
- âœ… Complete query pipeline (NLâ†’SQLâ†’executionâ†’results) - READY
- âœ… Database with seed data - READY
- âœ… LLM integration established - READY
- âœ… Frontend results display - READY

**Preparation Needed:**
- Install and configure Ragas Python library
- Research Ragas metrics configuration for NL-to-SQL context
- Design evaluation UI components
- Plan comparative reporting data structure

**Technical Prerequisites:**
- Ragas library compatibility with Python 3.11+
- LLM embeddings model selection for evaluation
- Logging infrastructure for query/score persistence

---

## PART 1: EPIC 001 REVIEW DISCUSSION

### Sarah (Product Owner)

**What Went Well:**
- All 8 stories delivered with comprehensive acceptance criteria validation - 100% AC coverage
- Multi-layered security implemented exactly as specified (5 layers: DB permissions, prompt engineering, sanitization, SQL validation, env protection)
- Senior Developer Review process caught quality issues early, preventing tech debt accumulation
- All 4 mandatory query types working end-to-end with real database integration

**What Could Improve:**
- Some stories had multiple review cycles (1.1 had 3 versions, 1.3 had 3 versions) - requirements could have been clearer upfront
- Cross-story schema coordination required careful tracking (Stories 1.2, 1.5, 1.6 dependencies)

**Lessons Learned:**
- The Senior Developer Review AI pattern is incredibly valuable - caught PropTypes missing, accessibility gaps, security vulnerabilities
- Acceptance criteria with specific test data requirements (">= 5 employees hired in last 6 months") prevented ambiguous "done" definitions
- Explicit integration checkpoints (Story 1.2 Task 8, Story 1.5 Task 7, Story 1.6 Task 8) ensured downstream stories had verified foundations

---

### Bob (Scrum Master)

**What Went Well:**
- Zero blocked stories - all dependencies resolved before starting dependent work
- Excellent test discipline - every story has passing tests before merge (100% test coverage achieved by Story 1.8)
- Integration checkpoints prevented rework - validating LLMâ†’DB alignment in Story 1.5 before building validation in 1.6
- Team velocity consistent across sprint despite technology learning curve (Ragas, NLâ†’SQL, GPT-5 Nano)

**What Could Improve:**
- Initial port binding blocker (Story 1.1) consumed significant time - Windows dev environment not validated in planning
- Some stories revealed missing architectural decisions (e.g., GPT-5 Nano model selection happened in Story 1.4 notes, not architecture phase)
- Review follow-ups sometimes created mini-sprints within stories (Story 1.3 had 8 follow-up action items)

**Lessons Learned:**
- The "ðŸ”— INTEGRATION CHECKPOINT" task pattern is brilliant - forces validation before proceeding
- Dev Agent Record documentation quality varied - Story 1.8 had exceptional root cause analysis, earlier stories less detailed
- Blockers were consistently documented but resolution time varied (session lifecycle bug took multiple analysis cycles)

---

### Amelia (Developer Agent)

**What Went Well:**
- SQLAlchemy 2.0 migration handled systematically - `result.mappings()` pattern applied consistently across codebase
- FastAPI async patterns implemented correctly - proper use of `asyncio.wait_for()`, timeout handling, lifespan context manager
- Security layering works beautifully - multi-layered defense prevented any SQL injection risks
- Test infrastructure solid - 51 unit tests + 19 integration tests all passing by Story 1.8

**What Could Improve:**
- Session lifecycle bug (Story 1.8) should have been caught earlier - anti-pattern of `try/finally` in `get_db_session()` existed from Story 1.2
- Deprecated patterns slipped through - `datetime.utcnow()` and `@app.on_event("startup")` flagged in reviews but initially merged
- Type hints inconsistent in early stories - improved over time but could have been enforced from Story 1.1

**Lessons Learned:**
- Tree of Thoughts elicitation (Story 1.8 v0.4) revealed completely incorrect root cause analysis - never trust first diagnosis
- GPT-5 Nano requires `reasoning_effort` and `verbosity` parameters - undocumented behavior discovered through systematic debugging
- FastAPI lifespan pattern is mandatory for production - old event hooks don't work with TestClient

---

### Murat (Master Test Architect)

**What Went Well:**
- Test coverage trajectory excellent - 0 tests (Story 1.1) â†’ 7 tests (1.2) â†’ 51 tests (1.3) â†’ 59 tests (1.6) â†’ 100% (1.8)
- Integration test design caught critical bugs - parental leave query returning 0 results revealed database connection issue
- Edge case coverage strong - NULL handling, Decimal serialization, timeout scenarios, 0-result queries all tested
- Performance validation included - P95 latency < 50ms verified for validation layer

**What Could Improve:**
- No load testing performed - connection pool exhaustion scenario documented but not automated (Story 1.7 Task 7.2)
- Frontend tests minimal until Story 1.3 - could have started test-driven development earlier
- Some tests mocked too aggressively - LLM service tests mock OpenAI entirely, missing real API integration validation

**Lessons Learned:**
- E2E tests reign supreme - Story 1.8 integration tests found bugs that unit tests missed (session lifecycle, Decimal serialization)
- Test data intentionality matters - "John Doe" as manager for 4+ employees (Story 1.2) enabled realistic query testing
- Test infrastructure needs equal attention to production code - `conftest.py` with DATABASE_URL override was critical

---

### Winston (Architect)

**What Went Well:**
- Modular monolith pattern worked perfectly - clear service boundaries (llm_service, validation_service, query_service) without over-engineering
- Connection pooling configuration optimal - 5 base + 15 overflow = 20 max concurrent, handled load well
- Security architecture sound - defense-in-depth with 5 layers, no single point of failure
- Technology stack choices validated - FastAPI async, SQLAlchemy 2.0, GPT-5 Nano all delivered as expected

**What Could Improve:**
- Static file serving path resolution fragile - BASE_DIR calculation may fail in containerized environments (flagged in Story 1.8 review)
- CORS configuration evolved across stories - should have been architected upfront with env-driven headers
- Timeout cascade not fully documented - API:10s, LLM:5s, DB:3s relationships emerged organically rather than by design

**Lessons Learned:**
- Lazy initialization pattern (Story 1.7) prevents import-time database connections - essential for testing
- FastAPI dependency injection (`Depends()`) not utilized - current manual session management works but not idiomatic
- SQLAlchemy 2.0 `result.mappings()` is the modern pattern - `row._mapping` causes cryptic errors

---

### Mary (Business Analyst)

**What Went Well:**
- Requirements traceability excellent - every AC cites source PRD/tech-spec with line numbers
- Test scenarios aligned with business value - 4 mandatory queries match real HR use cases
- Data model intentionally designed - seed data covers all test scenarios by design, not accident
- Error messaging user-friendly - generic messages to users, detailed logs for developers (Story 1.7)

**What Could Improve:**
- Some ACs discovered during implementation - client-side validation (Story 1.3 AC5) not in original PRD
- Requirements clarification needed mid-story - empty query handling (Story 1.8) required decision on 422 vs 200 response
- Cross-story coordination heavy - Schema dependencies between Stories 1.2, 1.5, 1.6 required careful synchronization

**Lessons Learned:**
- Explicit test data requirements in ACs prevent ambiguity - ">= 5 employees hired last 6 months" is better than "some recent hires"
- Integration checkpoints force requirements validation - Story 1.5 Task 7 validated LLM output against real seed data
- Senior Developer Review reveals hidden requirements - accessibility (ARIA labels), PropTypes, Error Boundaries emerged in reviews

---

## PART 2: EPIC 002 PREPARATION DISCUSSION

### Sarah (Product Owner)

**Dependencies Check:**
- Epic 001 delivers complete query pipeline - ready for evaluation layer âœ…
- Database with 52 employee records - sufficient for evaluation scenarios âœ…
- LLM integration working - can reuse for Ragas evaluator LLM âœ…

**Preparation Needs:**
- Research Ragas metrics configuration for NL-to-SQL domain (default metrics are for RAG systems)
- Design evaluation UI components (score display, color coding)
- Define acceptable score thresholds (currently PRD says 0.7+ acceptable, 0.8+ good)

**Risk Assessment:**
- Ragas may not directly support NL-to-SQL evaluation - may need custom metric implementations
- Evaluation LLM costs could be high if running on every query - need cost optimization strategy
- Frontend real estate limited - need clean design for displaying 3 metrics + explanations

---

### Bob (Scrum Master)

**Dependencies Check:**
- Story 1.8 delivered 100% functional end-to-end flow âœ…
- Logging infrastructure exists (structlog) - ready for query/score persistence âœ…
- API response model (QueryResponse) has room for evaluation scores âœ…

**Preparation Needs:**
- Install Ragas library and verify Python 3.11+ compatibility
- Create evaluation service module structure (similar to llm_service, validation_service pattern)
- Design data model for evaluation results storage

**Risk Assessment:**
- Ragas learning curve unknown - may require experimentation sprint before Story 2.1
- Integration with existing async FastAPI may have compatibility issues
- Performance impact of evaluation step - must stay within 5s total response time (NFR002)

---

### Amelia (Developer Agent)

**Dependencies Check:**
- All Epic 001 code merged and tested - clean slate for Epic 002 âœ…
- FastAPI lifespan pattern migrated (Story 1.8 v0.6) - ready for new startup initialization âœ…
- Pydantic models established - can extend QueryResponse easily âœ…

**Preparation Needs:**
- Research Ragas Python SDK and async compatibility
- Set up Ragas evaluator LLM configuration (separate from GPT-5 Nano query LLM)
- Create evaluation pipeline architecture design

**Risk Assessment:**
- Ragas may require synchronous execution - need async wrapper
- Embeddings model selection - cost vs quality tradeoff
- Error handling for evaluation failures - should not block query results

---

### Murat (Master Test Architect)

**Dependencies Check:**
- Test infrastructure mature - 70 total tests passing across backend âœ…
- Integration test pattern established (Story 1.8) - reusable for evaluation tests âœ…
- Mock fixtures in place - can mock Ragas evaluator for unit tests âœ…

**Preparation Needs:**
- Design test strategy for non-deterministic evaluation scores (acceptable range testing)
- Create test data with known-good and known-poor queries for evaluation validation
- Plan for evaluation performance testing

**Risk Assessment:**
- Ragas score variability - same query may get different scores on repeat evaluation
- Evaluation test flakiness - need to account for score ranges, not exact matches
- Cost of running Ragas in tests - may need aggressive mocking to avoid API costs

---

### Winston (Architect)

**Dependencies Check:**
- Service layer pattern established - evaluation fits as new service âœ…
- Async infrastructure solid - can integrate async evaluation calls âœ…
- Database ready for evaluation logs - can add new table for query history âœ…

**Preparation Needs:**
- Design evaluation pipeline flow: pre-query (context) vs post-query (results)
- Plan caching strategy for evaluation scores (avoid re-evaluating identical queries)
- Architect comparative report data structure

**Risk Assessment:**
- Evaluation latency may push total response time over 5s - need parallel execution or caching
- Ragas may not directly map to NL-to-SQL - need to understand context vs query vs answer definitions
- Report generation may require complex aggregations - consider separate async job

---

### Mary (Business Analyst)

**Dependencies Check:**
- 4 mandatory query types working - evaluation test dataset ready âœ…
- User journey defined in PRD - evaluation scores fit naturally below results table âœ…
- Success criteria clear - NFR004 specifies 0.7+ acceptable, 0.8+ good âœ…

**Preparation Needs:**
- Define evaluation metric interpretations for NL-to-SQL domain (Faithfulness = schema consistency?)
- Specify actionable recommendation templates (what to show users when scores are low)
- Clarify comparative report requirements - what insights are most valuable?

**Risk Assessment:**
- Ragas metrics may not be intuitive to users - need clear explanations
- Recommendation generation requires domain knowledge - may need manual curation initially
- Report scope could expand quickly - need MVP boundaries (Story 2.4)

---

## ACTION ITEMS

### Process Improvements

1. **Establish Windows dev environment validation checklist before future epics**
   Owner: Bob
   Timeline: Before Epic 2 start

2. **Enhance Dev Agent Record template with explicit root cause analysis section**
   Owner: Sarah
   Timeline: Epic 2 Story 2.1

3. **Add architecture decision log (ADR) for mid-sprint technical choices**
   Owner: Winston
   Timeline: Epic 2 planning

### Technical Debt

1. **Implement frontend unit tests for `api.js` service**
   Owner: Amelia
   Priority: MEDIUM

2. **Add connection pool load testing automation (Story 1.7 Task 7.2)**
   Owner: Murat
   Priority: LOW

3. **Consider FastAPI Depends() pattern refactor for session management**
   Owner: Winston
   Priority: LOW

### Documentation

1. **Create runbook for connection pool tuning and monitoring**
   Owner: Winston
   Timeline: Before production deployment

2. **Document GPT-5 Nano parameter requirements (reasoning_effort, verbosity)**
   Owner: Amelia
   Timeline: Epic 2 start

### Team Agreements

- All stories must have integration checkpoint tasks for cross-story dependencies
- Senior Developer Review occurs before marking story "Done" - no exceptions
- Blockers documented in real-time with elicitation methods applied for diagnosis
- Test coverage expectations: 80%+ for services, 100% for integration flows

---

## EPIC 002 PREPARATION SPRINT

### Technical Setup
- [ ] Install Ragas Python library and verify compatibility (Owner: Amelia, Est: 2 hours)
- [ ] Research Ragas NL-to-SQL configuration patterns (Owner: Winston, Est: 4 hours)
- [ ] Set up evaluation LLM and embeddings model (Owner: Amelia, Est: 2 hours)
- [ ] Create evaluation service module structure (Owner: Amelia, Est: 1 hour)

### Knowledge Development
- [ ] Research Ragas Faithfulness metric for NL-to-SQL context (Owner: Mary, Est: 3 hours)
- [ ] Research Ragas Answer Relevance customization (Owner: Mary, Est: 2 hours)
- [ ] Spike: Test Ragas with sample HR queries (Owner: Amelia, Est: 4 hours)
- [ ] Define score interpretation thresholds (Owner: Sarah + Mary, Est: 2 hours)

### Documentation
- [ ] Create Epic 002 tech spec with Ragas integration architecture (Owner: Winston, Est: 3 hours)
- [ ] Document evaluation pipeline design (Owner: Winston, Est: 2 hours)
- [ ] Define evaluation UI component requirements (Owner: Sarah, Est: 2 hours)

**Total Estimated Effort:** ~30 hours (~4 days at 8 hours/day)

---

## KEY TAKEAWAYS

1. **Multi-layered security architecture delivered exactly as designed** - zero injection vulnerabilities
2. **Senior Developer Review AI pattern caught 15+ quality issues** before production
3. **Integration checkpoints prevented costly rework** - systematic validation between dependent stories
4. **Tree of Thoughts elicitation revealed incorrect diagnoses** - systematic debugging essential
5. **Test coverage discipline paid off** - 100% coverage caught critical bugs early

---

## NEXT STEPS

1. Execute Preparation Sprint (Est: 4 days)
2. Review action items in next standup
3. Begin Epic 002 planning when preparation complete

---

**Bob**: "Great work team! We learned a lot from Epic 001. The systematic debugging using Tree of Thoughts, the comprehensive test coverage, and the security-first architecture all demonstrate exceptional engineering discipline. Let's use these insights to make Epic 002 even better. See you at sprint planning once prep work is done!"
